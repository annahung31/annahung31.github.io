---
permalink: /
title: "Hsiao-Tzu (Anna) Hung"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a master student at National Taiwan University, Taiwan. My main research interest is deep learning on multimedia, especially on audios and images. Under the supervision of Dr. Yi-Hsuan Yang (Academia Sinica), I'm now doing an interesting research on automaic music generation based on transformer-based model. Currently I'm seeking for ML/AI/Data/software engineering Full-time position. Please reach me if you have anything to share :) 

[RESUME](https://annahung31.github.io/files/Anna_resume_2021_v3.pdf)


Education
======

* M.S. in Department of CSIE,  
National Taiwan University, Taiwan, present
* B.S. in Department of Physics,  
National Tsing Hua University, Taiwan, 2014



Work experiences
======

2021 June - Aug  
*Acoustic Engineering Intern*  
Amazon Ring, Taiwan


2020 - current  
*Research Assistant*  
Institute of Information Science, Academia Sinica, Taiwan  
supervised by Dr. Yi-Hsuan Yang: [Music and AI Lab](https://musicai.citi.sinica.edu.tw/)


2019 - 2020  
*Full-time Machine Learning Research Internship*  
Taiwan AI LAbs, Taiwan  

2018 - 2019  
*Research Assistant*  
Institute of Information Science, Academia Sinica, Taiwan  
supervised by Dr. Hsin-Min Wang: Speech,Language and Music Processing Laboratory  

2017 - 2018  
*Physics teacher*  
National Lan-Yang Girls’ Senior High School  

2014 - 2016  
*Physics teacher*  
National Chu-Pei Senior High School  



Publications
======

<font color="#006600">EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation</font>  
**Hsiao-Tzu Hung**, Joann Ching, Seungheon Doh, Nabin Kim, Juhan Nam and Yi-Hsuan Yang  
Published on <font color="#008080">ISMIR 2021</font>  
[Paper](), [Demo](https://annahung31.github.io/EMOPIA/), [Code](https://github.com/annahung31/EMOPIA)    
We collected a dataset called "EMOPIA", which is a shared multi-modal (audio and MIDI) database focusing on perceived emotion in pop piano music, to facilitate research on various tasks related to music emotion. I also made an attempt to control the emotion of the generated piano music generated by Transformer using this dataset.  


<font color="#006600">Improving automatic Jazz melody generation by transfer learning techniques</font>  
**Hsiao-Tzu Hung**, Chung-Yang Wang, Yi-Hsuan Yang, Hsin-Min Wang  
Published on <font color="#008080">APSIPA 2019</font>.  
[Paper](https://arxiv.org/abs/1908.09484), [Demo](https://annahung31.github.io/Publication-Demos/publications/jazz_melody_generation/), [Code](https://github.com/annahung31/jazz_melody_generation)    
In this paper, I use two transfer learning methods to improve the performance of a VAE-based music generation model given the limited training data. Both the objective and subjective test shows that the two methods indeed improve the performace.


<font color="#006600">MediaEval 2019 Emotion and Theme Recognition task: A VQ-VAE Based Approach</font>  
**Hsiao-Tzu Hung**, Yu-Hua Chen, Maximilian Mayer,Michael V¨otter, Eva Zangerle, Yi-Hsuan Yang  
Published on <font color="#008080">MediaEval 2019</font> .  
[Paper](https://evazangerle.at/publication/mediaeval-19-tai/mediaeval-19-tai.pdf), [Code](https://github.com/annahung31/moodtheme-tagging)  
In this work, we try to use the VQ-VAE as feature extractor and two kinds of classifier to automatically classify the genre, theme, or mood of a given audio song. The dataset given by the host is quiet noisy and so we decide not to move forward on this task, but still it's an interesting experience.


Mini Projects
======
<font color="#006600">Implementing MidiNet by PyTorch</font>  
[Code](https://github.com/annahung31/MidiNet-by-pytorch)  
In the beginning of my ML journey, I start by translating MidiNet from TensorFlow to PyTorch. MidiNet is an well-known GAN-based framework in music automatic generaion field. This project currently has 40 stars on GitHub. 

<font color="#006600">covid19-prediction</font>    
[Code](https://github.com/annahung31/covid19-prediction)  
This is a small project I've done when I'm taking the course "Artificial intelligence" @ NTU. I use X-ray as input to predict covid19. In this work, I try to use focal loss to deal with data imbalance.




More about me
======  
Outside of work, I'm a scuba diving lover, a book addict, and a Spotify loyal user!


<style>

img {
  border-radius: 5%;
}
.resp-iframe {
  position: relative;
  top: 10;
  left: 0;
  width: 60%;
  height: 142;
  border-radius: 10px;
}
</style>

Diving in sea of Kenting, Taiwan (A must-visit place!):  
<br>
<img src="https://raw.githubusercontent.com/annahung31/annahung31.github.io/academic/images/diving.JPG" width=290 top=10 border-radius=10px>
<br><br>
My current favorite playlist:  
 <iframe class="resp-iframe" src="https://open.spotify.com/embed/playlist/37i9dQZF1DX889U0CL85jj?theme=0" width="30%" height="200" frameBorder="0" allowtransparency="true" allow="encrypted-media"></iframe>


